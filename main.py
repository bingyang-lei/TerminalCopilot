import os
import argparse
import sys
import warnings
from utils.ArgParser import argparser
from openai import OpenAI
from openai.types.chat import ChatCompletionChunk
args = argparser()

url1 = 'http://localhost:11434/v1/'
url2 = "http://localhost:11435/v1/"  # å¦‚æœé‡‡ç”¨å¤šæ¨¡å‹å¯¹è¯åˆ™è¦ä½¿ç”¨å¤šä¸ªç«¯å£,å¹¶export OLLAMA_HOST="0.0.0.0:11435"

client = OpenAI(
    base_url = url1,
    api_key='ollama', # required, but unused
)
# llama3.2:1b, llama3.1:8b, qwen2.5, deepseek-r1:32b
REASONING_MODEL = 'deepseek-r1:32b'  # æ¨ç†æ¨¡å¼ä½¿ç”¨æ¨¡å‹
FINALMODEL = REASONING_MODEL if args.reasoning else args.model
FINALMODEL = args.model
PPT_SYSTEM_PROMPT = "ä¸‹é¢è¯·ä½ æ ¹æ®ç”¨æˆ·çš„æè¿°ï¼Œç”Ÿæˆç¬¦åˆè¦æ±‚çš„PPTï¼Œè¾“å‡ºä¸ºmarkdownæ ¼å¼ï¼Œéµå¾ªreveal.jsçš„è¦æ±‚ã€‚"
conversation_history = [{"role": "system", "content": "You are a helpful assistant."}]

def talk_mode(initial_prompt: str, max_rounds: int, model1: str, model2: str = None):
    """
    å¯¹è¯æ¨¡å¼æ ¸å¿ƒé€»è¾‘ï¼ˆæ”¯æŒåŒæ¨¡å‹å¯¹è¯ï¼‰
    """
    global conversation_history
    
    current_model = model1
    current_content = initial_prompt
    conversation_history = []  # é‡ç½®å¯¹è¯å†å²
    
    for round in range(max_rounds):
        print(f"\n\U0001F4E9 Round {round+1}/{max_rounds}")
        
        # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°å¯¹è¯å†å²
        conversation_history.append({"role": "user", "content": current_content})
        
        try:
            # åˆ›å»ºèŠå¤©å®Œæˆè¯·æ±‚
            stream = client.chat.completions.create(
                model=current_model,
                messages=conversation_history,
                temperature=0.7,
                max_tokens=4096,
                stream=True
            )
            
            # å¤„ç†æµå¼å“åº”
            full_response, current_content = stream_response(stream, current_model)
            
            # æ›´æ–°å¯¹è¯å†å²
            conversation_history.append({"role": "assistant", "content": full_response})
            
            # åˆ‡æ¢æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨åŒæ¨¡å‹ï¼‰
            if model2:
                current_model = model2 if current_model == model1 else model1
                client.base_url = url2 if client.base_url == url1 else url1
        
        except Exception as e:
            print(f"\n\033[31mError: {str(e)}\033[0m")
            break

def stream_response(stream, current_model=None, display=True):
    full_response = ""
    global FINALMODEL
    if current_model:
        FINALMODEL = current_model
    if display:
        print(f"\nğŸ¤– {FINALMODEL}:", end="", flush=True)
    
    try:
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                if display:
                    print(content, end="", flush=True)
                full_response += content
        if display:
            print("\n")
        return full_response, full_response  # è¿”å›å®Œæ•´å“åº”å’Œä¸‹ä¸€è½®è¾“å…¥
    except Exception as e:
        print(f"\n\033[31mStream Error: {str(e)}\033[0m")
        return full_response, full_response

def main():
    global conversation_history
    pipe_input = ""
    if not sys.stdin.isatty():
        pipe_input = str(sys.stdin.buffer.read())
        print(f"æ¥å—ç®¡é“è¾“å…¥æˆåŠŸ")
    tty_fd = os.open('/dev/tty', os.O_RDONLY)
    sys.stdin = os.fdopen(tty_fd, 'r')

    if args.ppt:
        conversation_history[0]["content"] = PPT_SYSTEM_PROMPT
    
    if args.talk:
        initial_prompt = args.talk[0]
        try:
            max_rounds = int(args.talk[1])
        except ValueError:
            print("\033[31mé”™è¯¯ï¼šè½®æ•°å¿…é¡»ä¸ºæ•´æ•°\033[0m")
            return
        
        model2 = args.model2 if args.model2 else None
        talk_mode(initial_prompt, max_rounds, FINALMODEL, model2)
    else:
        while True:
            try:
                user_input = input("\n\U0001F4AC Your question (exit to quit): ")
                if user_input.lower() in ["exit", "quit", "q"]:
                    break
                user_input = pipe_input + user_input
                pipe_input = ''
                # æ›´æ–°å¯¹è¯å†å²
                conversation_history.append({"role": "user", "content": user_input})

                if args.reasoning:  # æ–°å¢æ¨ç†æ¨¡å¼åˆ†æ”¯
                    # ç¬¬ä¸€é˜¶æ®µï¼šç”Ÿæˆæ€è€ƒæ­¥éª¤
                    user_input + 'æˆ‘çš„é—®é¢˜å¦‚ä¸Šæ‰€ç¤ºï¼Œè¯·ä½ åˆ†æ­¥åˆ†æå¹¶è®¤çœŸæ€è€ƒæˆ‘çš„éœ€æ±‚ï¼Œè¾“å‡ºæ¸…æ™°çš„è§£å†³æ­¥éª¤ï¼ˆä¸è¦ç›´æ¥å›ç­”ï¼‰'
                    conversation_history.append({"role": "user", "content": user_input})
                    print(f"\nâ° æ€è€ƒä¸­...")
                    stream = client.chat.completions.create(
                        model=FINALMODEL,
                        messages=conversation_history,
                        temperature=0.7,
                        max_tokens=4096,
                        stream=True
                    )
                    step1_response, _ = stream_response(stream,display=False)
                    print(f"âœ¨ æ€è€ƒç»“æŸ âœ¨")
                    # ç¬¬äºŒé˜¶æ®µï¼šç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
                    conversation_history.extend([
                        {"role": "assistant", "content": step1_response},
                        {"role": "user", "content": "è¯·åŸºäºä¸Šè¿°æ€è€ƒæ­¥éª¤ï¼Œç»™å‡ºä¸“ä¸šä¸¥è°¨çš„æœ€ç»ˆè§£ç­”"}
                    ])
                    
                    stream = client.chat.completions.create(
                        model=FINALMODEL,
                        messages=conversation_history,
                        temperature=0.5, # ç¬¬äºŒé˜¶æ®µï¼Œé™ä½æ¸©åº¦ï¼Œä»¥è·å¾—æ›´ç¨³å®šçš„å›ç­”
                        max_tokens=4096,
                        stream=True
                    )
                    step2_response, _ = stream_response(stream)
                    conversation_history.append({"role": "assistant", "content": step2_response})
                else:
                    # åˆ›å»ºèŠå¤©å®Œæˆè¯·æ±‚
                    stream = client.chat.completions.create(
                        model=FINALMODEL,
                        messages=conversation_history,
                        temperature=0.7,
                        max_tokens=4096,
                        stream=True
                    )
                    
                    # å¤„ç†æµå¼å“åº”
                    full_response, _ = stream_response(stream)
                    
                    # æ›´æ–°å¯¹è¯å†å²
                    conversation_history.append({"role": "assistant", "content": full_response})
            
            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"\n\033[31mError: {str(e)}\033[0m")
                break

if __name__ == "__main__":
    main()