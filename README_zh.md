# TCP: ç»ˆç«¯ Copilot ğŸš€

ä½ çš„ AI å‘½ä»¤è¡Œä¼´ä¾£ã€‚
(this file is generated from `cat README.md | tcp`)

<details>
    <summary>Demo</summary>
    <img src="./photos/show.gif" alt="tcp Demo">
</details>

åŸºäº **Unix å“²å­¦** æ„å»ºï¼Œä½ å¯ä»¥é€šè¿‡ç®¡é“ä¼ é€’ä»»ä½•å†…å®¹ã€‚

![ç¤ºä¾‹](./photos/image.png)

ï¼ˆè¯´å®è¯ï¼Œæˆ‘è§‰å¾—è¿™ä¸ª README æœ‰ç‚¹å“—ä¼—å–å® äº†ï¼ŒTCP è¿˜æœ‰è®¸å¤šåŠŸèƒ½éœ€è¦æ·»åŠ æˆ–æ”¹è¿›ï¼Œä½†æ˜¾ç„¶AIè‡ªæˆ‘æ„Ÿè§‰è‰¯å¥½ğŸ¥ºï¼‰

## ğŸš€ åŠŸèƒ½

### å®šåˆ¶çš„AIåŠ©æ‰‹
- **å¤šæ¨¡å‹é€‰æ‹©**  
  `-m qwen2.5` | `--model deepseek-r1:32b`
  åˆ‡æ¢ä¸åŒçš„ LLM , å°±åƒåˆ‡æ¢ shell
- **æ¨ç†æ¨¡å¼** (`-r`)
  ä¸¤é˜¶æ®µæ‰¹åˆ¤æ€§æ€ç»´ï¼š  
  `åˆ†æ â†’ æ‰§è¡Œ`ï¼Œä¸¤é˜¶æ®µé‡‡ç”¨ä¸åŒå‚æ•°è®¾ç½®
- **PPT ç”Ÿæˆæ¨¡å¼** (`-p`)  
  åœ¨ç»ˆç«¯ç”Ÿæˆæ¼”ç¤ºæ–‡ç¨¿çº§çš„ markdown
- **åŒæ¨¡å‹å¯¹è¯** (`--model2 llama3.1:8b`)  
  æ¨¡å‹ vs æ¨¡å‹çš„è¾©è®º
- **åŠ å…¥å…¶ä»–ä½ è‡ªå·±çš„åŠŸèƒ½**  
  åŸºäº TCP æ„å»ºä½ çš„ä¸ªæ€§åŒ– AI åŠ©æ‰‹

### **åŸç”Ÿç»ˆç«¯é­”æ³•**
- **ç®¡é“åŠ›é‡**  
  ```bash
  ls -la | tcp -m qwen2.5 "è§£é‡Šè¿™äº›æ–‡ä»¶æƒé™"
  man grep | tcp -r "æ€»ç»“å…³é”®æ ‡å¿—"
  ```
- **å¯¹è¯æ¨¡å¼** (`-t`)  
  ```bash
  tcp -t "è°ƒè¯•è¿™ä¸ª Python è„šæœ¬" 5
  ```
- **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**
  ç»´æŠ¤ä¼šè¯å†å²ï¼Œåƒä½ æœ€å–œæ¬¢çš„ shell é‚£æ ·

## ğŸ› ï¸ å®‰è£…

### **å…ˆå†³æ¡ä»¶**
- Python 3.9+
- [Ollama](https://ollama.ai/) è¿è¡Œä¸­

```bash
# å¿«é€Ÿå…‹éš†
git clone https://github.com/bingyang-lei/TerminalCopilot.git && cd TerminalCopilot

# å®‰è£…ä¾èµ–ï¼ˆæ¨èä½¿ç”¨ virtualenvï¼‰
pip install -r requirements.txt

# å¯åŠ¨ AI å¼•æ“
ollama serve &  # åœ¨åå°è¿è¡Œ

# æ‹‰å–ä½ å–œæ¬¢çš„æ¨¡å‹
ollama pull qwen2.5 deepseek-r1:32b

# ï¼ˆå¯é€‰ï¼‰è®¾ç½®åˆ«å
echo 'alias tcp="python /yourpath/main.py"' >> ~/.bashrc
source ~/.bashrc
```

## ğŸ“š ç”¨æ³•

<details>
<summary>å‘½ä»¤è¡Œå‚æ•°</summary>

```commandline
usage: main.py [-h] [-m MODEL] [--model2 MODEL2] [-r] [-p] [-t PROMPT ROUNDS]

options:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Designate a model to use, default is qwen2.5
  --model2 MODEL2       (option) Choosing another model to talk with final model
  -r, --reasoning       Enable reasoning mode(more powerful but slower)
  -p, --ppt             enter PPT generation mode
  -t PROMPT ROUNDS, --talk PROMPT ROUNDS
                        Start dialogue mode with initial prompt and max rounds
```
</details>

## ğŸ¤” ä¸ºä»€ä¹ˆé€‰æ‹© tcpï¼Ÿ

### **Unix å“²å­¦çš„ä½“ç°**
```bash
# é“¾æ¥åˆ°ç»å…¸å·¥å…·
find . -name "*.py" | tcp -r "åˆ†æä»£ç æ¨¡å¼"
netstat -tulpn | tcp "è§£é‡Šè¿™äº›ç½‘ç»œè¿æ¥"
man sh | tcp "ç»™æˆ‘æ›´å‹å¥½çš„æ‰‹å†Œ"
```
---

<!-- **çŒ®ç»™ç»ˆç«¯æˆ˜å£«ä»¬**  \n[è´¡çŒ®](#) | [æ–‡æ¡£](#) | [èµåŠ©](#) -->